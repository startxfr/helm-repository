{{- if .Values.kraken.enabled -}}
{{- $appName := include "startx.appName" . -}}
{{- $namespace := .Values.kraken_project.project.name -}}
{{- if .Values.kraken.aws -}}{{- if .Values.kraken.aws.enabled -}}
{{- with .Values.kraken.aws }}
---
kind: Secret
apiVersion: v1
type: Opaque
metadata:
  name: kraken-aws-creds
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-aws-creds"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
stringData:
  AWS_ACCESS_KEY_ID: {{ .credentials.key_id | default "AWS_ACCESS_KEY_ID" | quote }}
  AWS_DEFAULT_REGION: {{ .credentials.region | default "AWS_DEFAULT_REGION" | quote }}
  AWS_SECRET_ACCESS_KEY: {{ .credentials.secret | default "AWS_SECRET_ACCESS_KEY" | quote }}
{{ end }}
{{- end -}}{{- end -}}
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-scenarios-config
  namespace: "{{- $namespace -}}"
  labels:
    test: essai
    app.kubernetes.io/name: "kraken-scenarios-config"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  app_outage.yaml: |
    application_outage:                                  # Scenario to create an outage of an application by blocking traffic
      duration: 600                                      # Duration in seconds after which the routes will be accessible
      namespace: <namespace-with-application>            # Namespace to target - all application routes will go inaccessible if pod selector is empty
      pod_selector: {app: foo}                            # Pods to target
      block: [Ingress, Egress]                           # It can be Ingress or Egress or Ingress, Egress
  cluster_shut_down_scenario.yml: |
    cluster_shut_down_scenario:                          # Scenario to stop all the nodes for specified duration and restart the nodes
      runs: 1                                            # Number of times to execute the cluster_shut_down scenario
      shut_down_duration: 120                            # duration in seconds to shut down the cluster
      cloud_type: aws                                    # cloud type on which Kubernetes/OpenShift runs
      timeout: 120                                       # Number of seconds to wait for each node to be stopped or running
  customapp_pod.yaml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 30
        minSecondsBetweenRuns: 1
    scenarios:
      - name: "delete acme-air pods"
        steps:
        - podAction:
            matches:
              - labels:
                  namespace: "acme-air"
                  selector: ""
            filters:
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
        - podAction:
            matches:
              - labels:
                  namespace: "acme-air"
                  selector: ""
            retries:
              retriesTimeout:
                timeout: 180

            actions:
              - checkPodCount:
                  count: 8
  post_action_etcd_container.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time

    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out


    i = 0
    while i < 100:
        pods_running = run("oc get pods -n openshift-etcd -l app=etcd | grep -c '4/4'").rstrip()
        if pods_running == "3":
            break
        time.sleep(5)
        i += 1

    if pods_running == str(3):
        print("There were 3 pods running properly")
    else:
        print("ERROR there were " + str(pods_running) + " pods running instead of 3")
  post_action_etcd_example_py.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging


    def run(cmd):
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
            logging.info("out " + str(out))
        except Exception as e:
            logging.error("Failed to run %s, error: %s" % (cmd, e))
        return out


    pods_running = run("oc get pods -n openshift-etcd | grep -c Running").rstrip()

    if pods_running == str(3):
        print("There were 3 pods running properly")
    else:
        print("ERROR there were " + str(pods_running) + " pods running instead of 3")
  post_action_shut_down.py: |
    #!/usr/bin/env python3
    import subprocess
    import logging
    import time
    import yaml


    def run(cmd):
        out = ""
        try:
            output = subprocess.Popen(
                cmd, shell=True, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT
            )
            (out, err) = output.communicate()
        except Exception as e:
            logging.info("Failed to run %s, error: %s" % (cmd, e))
        return out


    # Get cluster operators and return yaml
    def get_cluster_operators():
        operators_status = run("kubectl get co -o yaml")
        status_yaml = yaml.load(operators_status, Loader=yaml.FullLoader)
        return status_yaml


    # Monitor cluster operators
    def monitor_cluster_operator(cluster_operators):
        failed_operators = []
        for operator in cluster_operators["items"]:
            # loop through the conditions in the status section to find the dedgraded condition
            if "status" in operator.keys() and "conditions" in operator["status"].keys():
                for status_cond in operator["status"]["conditions"]:
                    # if the degraded status is not false, add it to the failed operators to return
                    if status_cond["type"] == "Degraded" and status_cond["status"] != "False":
                        failed_operators.append(operator["metadata"]["name"])
                        break
            else:
                logging.info("Can't find status of " + operator["metadata"]["name"])
                failed_operators.append(operator["metadata"]["name"])
        # return False if there are failed operators else return True
        return failed_operators


    wait_duration = 10
    timeout = 900
    counter = 0

    counter = 0
    co_yaml = get_cluster_operators()
    failed_operators = monitor_cluster_operator(co_yaml)
    while len(failed_operators) > 0:
        time.sleep(wait_duration)
        co_yaml = get_cluster_operators()
        failed_operators = monitor_cluster_operator(co_yaml)
        if counter >= timeout:
            print("Cluster operators are still degraded after " + str(timeout) + "seconds")
            print("Degraded operators " + str(failed_operators))
            exit(1)
        counter += wait_duration

    not_ready = run("oc get nodes --no-headers | grep 'NotReady' | wc -l").rstrip()
    while int(not_ready) > 0:
        time.sleep(wait_duration)
        not_ready = run("oc get nodes --no-headers | grep 'NotReady' | wc -l").rstrip()
        if counter >= timeout:
            print("Nodes are still not ready after " + str(timeout) + "seconds")
            exit(1)
        counter += wait_duration

    worker_nodes = run("oc get nodes --no-headers | grep worker | egrep -v NotReady | awk '{print $1}'").rstrip()
    print("Worker nodes list \n" + str(worker_nodes))
    master_nodes = run("oc get nodes --no-headers | grep master | egrep -v NotReady | awk '{print $1}'").rstrip()
    print("Master nodes list \n" + str(master_nodes))
    infra_nodes = run("oc get nodes --no-headers | grep infra | egrep -v NotReady | awk '{print $1}'").rstrip()
    print("Infra nodes list \n" + str(infra_nodes))
  startx_pvc_fruitapp1-preprod-postgresql.yaml: |
    pvc_scenario:
      pvc_name: fruitapp1-preprod-postgresql
      namespace: fruitapp1-preprod
      fill_percentage: 99
      duration: 30
  startx_pvc_fruitapp2-prod-postgresql.yaml: |
    pvc_scenario:
      pvc_name: fruitapp2-prod-postgresql
      namespace: fruitapp2-prod
      fill_percentage: 99
      duration: 30
  startx_node_app_cpu_chaos.yaml: |
    apiVersion: litmuschaos.io/v1alpha1
    kind: ChaosEngine
    metadata:
      name: nginx-chaos
      namespace: litmus
    spec:
      annotationCheck: 'false'
      engineState: 'active'
      chaosServiceAccount: litmus-sa
      monitoring: false
      jobCleanUpPolicy: 'delete'
      experiments:
        - name: node-cpu-hog
          spec:
            components:
              env:
                - name: TOTAL_CHAOS_DURATION
                  value: '60'
                - name: NODE_CPU_CORE
                  value: '1'
                - name: NODES_AFFECTED_PERC
                  value: '75'
                - name: NODE_LABEL
                  value: 'node-role.kubernetes.io/app'
  startx_node_app_network_chaos.yaml: |
    network_chaos:
      duration: 30
      node_name: 
      label_selector: 'node-role.kubernetes.io/app'
      instance_count: 1
      interfaces:
      - "tun0"
      - "br0"
      - "lo"
      - "ens5"
      execution: serial
      egress:
        latency: 50ms  
        loss: 0.05
        bandwidth: 100mbit
  startx_node_master_network_chaos.yaml: |
    network_chaos:
      duration: 30
      node_name: 
      label_selector: 'node-role.kubernetes.io/master'
      instance_count: 1
      interfaces:
      - "tun0"
      - "br0"
      - "lo"
      - "ens5"
      execution: serial
      egress:
        latency: 50ms  
        loss: 0.05
        bandwidth: 100mbit
  startx_node_infra_network_chaos.yaml: |
    network_chaos:
      duration: 30
      node_name: 
      label_selector: 'node-role.kubernetes.io/infra'
      instance_count: 1
      interfaces:
      - "tun0"
      - "br0"
      - "lo"
      - "ens5"
      execution: serial
      egress:
        latency: 50ms  
        loss: 0.05
        bandwidth: 100mbit
  startx_node_master.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 60
        minSecondsBetweenRuns: 30
    node_scenarios:
      - actions:
          - stop_kubelet_scenario
          - start_kubelet_scenario
        node_name:
        label_selector: node-role.kubernetes.io/master
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: generic 
      - actions:
          - node_crash_scenario
        node_name:
        label_selector: node-role.kubernetes.io/master
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: generic 
      - actions: 
          - node_start_scenario
          - node_stop_start_scenario
          - node_termination_scenario
          - node_reboot_scenario
          - stop_kubelet_scenario
          - stop_start_kubelet_scenario
          - node_crash_scenario
          - stop_start_helper_node_scenario
        node_name:
        label_selector: node-role.kubernetes.io/master
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: aws 
      - actions:
        - node_reboot_scenario
        node_name:
        label_selector: node-role.kubernetes.io/master
        instance_count: 1
        timeout: 300
        cloud_type: aws
  startx_node_infra.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 60
        minSecondsBetweenRuns: 30
    node_scenarios:
      - actions:
          - stop_kubelet_scenario
          - start_kubelet_scenario
        node_name:
        label_selector: node-role.kubernetes.io/infra
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: generic 
      - actions:
          - node_crash_scenario
        node_name:
        label_selector: node-role.kubernetes.io/infra
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: generic 
      - actions: 
          - node_start_scenario
          - node_stop_start_scenario
          - node_termination_scenario
          - node_reboot_scenario
          - stop_kubelet_scenario
          - stop_start_kubelet_scenario
          - node_crash_scenario
          - stop_start_helper_node_scenario
        node_name:
        label_selector: node-role.kubernetes.io/infra
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: aws 
      - actions:
        - node_reboot_scenario
        node_name:
        label_selector: node-role.kubernetes.io/infra
        instance_count: 1
        timeout: 300
        cloud_type: aws
  startx_node_app.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 40
        minSecondsBetweenRuns: 20
    node_scenarios:
      - actions:
          - stop_kubelet_scenario
          - start_kubelet_scenario
        node_name:
        label_selector: node-role.kubernetes.io/app
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: generic 
      - actions:
          - node_crash_scenario
        node_name:
        label_selector: node-role.kubernetes.io/app
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: generic 
      - actions: 
          - node_start_scenario
          - node_stop_start_scenario
          - node_termination_scenario
          - node_reboot_scenario
          - stop_kubelet_scenario
          - stop_start_kubelet_scenario
          - node_crash_scenario
          - stop_start_helper_node_scenario
        node_name:
        label_selector: node-role.kubernetes.io/app
        instance_count: 1
        runs: 1
        timeout: 300 
        cloud_type: aws 
      - actions:
        - node_reboot_scenario
        node_name:
        label_selector: node-role.kubernetes.io/app
        instance_count: 1
        timeout: 300
        cloud_type: aws
  startx_openshift-apiserver.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 30
        minSecondsBetweenRuns: 1
    scenarios:
      - name: "delete openshift-apiserver pods"
        steps:
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-apiserver"
                  selector: "app=openshift-apiserver-a"
            filters:
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-apiserver"
                  selector: "app=openshift-apiserver-a"
            retries:
              retriesTimeout:
                timeout: 180
            actions:
              - checkPodCount:
                  count: 3
      - name: "delete openshift-kube-apiserver pods"
        steps:
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-kube-apiserver"
                  selector: "app=openshift-kube-apiserver"
            filters:
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-kube-apiserver"
                  selector: "app=openshift-kube-apiserver"
            retries:
              retriesTimeout:
                timeout: 180
            actions:
              - checkPodCount:
                  count: 3
  startx_openshift_etcd_container.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 10
        minSecondsBetweenRuns: 5
    scenarios:
      - name: "kill etcd container in etcd pod"
        namespace: "openshift-etcd"
        label_selector: "k8s-app=etcd"
        container_name: "etcd"
        action: "kill 1"
        count: 1
        retry_wait: 90
  startx_openshift_etcd_pod.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 10
        minSecondsBetweenRuns: 5
    scenarios:
      - name: "delete etcd pods"
        steps:
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-etcd"
                  selector: "k8s-app=etcd"
            filters:
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-etcd"
                  selector: "k8s-app=etcd"
            retries:
              retriesTimeout:
                timeout: 180
            actions:
              - checkPodCount:
                  count: 3
      - name: "delete etcd-quorum-guard pods"
        steps:
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-etcd"
                  selector: "k8s-app=etcd-quorum-guard"
            filters:
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
        - podAction:
            matches:
              - labels:
                  namespace: "openshift-etcd"
                  selector: "k8s-app=etcd-quorum-guard"
            retries:
              retriesTimeout:
                timeout: 180
            actions:
              - checkPodCount:
                  count: 3
  startx_openshift_logging.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 10
        minSecondsBetweenRuns: 5
    scenarios:
      - name: kill 2 collector pod in openshift-logging namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-logging"
              - labels:
                  namespace: "openshift-logging"
                  selector: "component=collector"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 2
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill 1 elasticsearch pod in openshift-logging namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-logging"
              - labels:
                  namespace: "openshift-logging"
                  selector: "component=elasticsearch"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill up to 5 running pods in openshift-logging namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-logging"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 4
            actions:
              - kill:
                  probability: .75
  startx_openshift_machines.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 10
        minSecondsBetweenRuns: 5
    scenarios:
      - name: kill 1 server pod in openshift-machine-config-operator namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-config-operator"
              - labels:
                  namespace: "openshift-machine-config-operator"
                  selector: "k8s-app=machine-config-server"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill up to 3 daemon running pods in openshift-machine-config-operator namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-config-operator"
              - labels:
                  namespace: "openshift-machine-config-operator"
                  selector: "k8s-app=machine-config-daemon"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 3
            actions:
              - kill:
                  probability: .75
      - name: kill up to 4 running pods in openshift-machine-* namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-*"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 4
            actions:
              - kill:
                  probability: .75
      - name: kill 1 pod in openshift-machine-api namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-machine-api"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: .7
  startx_openshift_monitoring.yml: |
    config:
      runStrategy:
        runs: 1
        maxSecondsBetweenRuns: 10
        minSecondsBetweenRuns: 5
    scenarios:
      - name: kill 1 Prometeus pod in openshift-monitoring namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-monitoring"
              - labels:
                  namespace: "openshift-monitoring"
                  selector: "app.kubernetes.io/name=prometheus"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill 2 node-exporter pod in openshift-monitoring namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-monitoring"
              - labels:
                  namespace: "openshift-monitoring"
                  selector: "app.kubernetes.io/name=node-exporter"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 2
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill 1 alertmanager pod in openshift-monitoring namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-monitoring"
              - labels:
                  namespace: "openshift-monitoring"
                  selector: "app.kubernetes.io/name=alertmanager"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill 1 thanos-querier pod in openshift-monitoring namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-monitoring"
              - labels:
                  namespace: "openshift-monitoring"
                  selector: "app.kubernetes.io/name=thanos-query"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 1
            actions:
              - kill:
                  probability: 1
                  force: true
      - name: kill up to 5 pods in openshift-monitoring namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-monitoring"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 5
            actions:
              - kill:
                  probability: .75
  startx_openshift_podmonkey.yml: |
    config:
      runStrategy:
        runs: 3
        maxSecondsBetweenRuns: 40
        minSecondsBetweenRuns: 20
    scenarios:
      - name: kill up to 5 pods in any openshift namespace
        steps:
        - podAction:
            matches:
              - namespace: "openshift-.*"
            filters:
              - property:
                  name: "state"
                  value: "Running"
              - randomSample:
                  size: 5
            actions:
              - kill:
                  probability: .75
  startx_time_node.yml: |
    time_scenarios:
      - action: skew_date
        object_type: node
        label_selector: node-role.kubernetes.io/master
      - action: skew_date
        object_type: node
        label_selector: node-role.kubernetes.io/infra
      - action: skew_date
        object_type: node
        label_selector: node-role.kubernetes.io/app
      - action: skew_time
        object_type: node
        label_selector: node-role.kubernetes.io/master
      - action: skew_time
        object_type: node
        label_selector: node-role.kubernetes.io/infra
      - action: skew_time
        object_type: node
        label_selector: node-role.kubernetes.io/app
  startx_time_pod.yml: |
    time_scenarios:
      - action: skew_date
        object_type: pod
        namespace: openshift-etcd
        container_name: etcd
        label_selector: app=etcd
      - action: skew_date
        object_type: pod
        namespace: openshift-machine-config-operator
        container_name: 
        label_selector: "k8s-app=machine-config-daemon"
      - action: skew_date
        object_type: pod
        namespace: openshift-apiserver
        container_name: 
        label_selector: "app=openshift-apiserver-a"
      - action: skew_time
        object_type: pod
        namespace: openshift-etcd
        container_name: etcd
        label_selector: app=etcd
      - action: skew_time
        object_type: pod
        namespace: openshift-machine-config-operator
        container_name: 
        label_selector: "k8s-app=machine-config-daemon"
      - action: skew_time
        object_type: pod
        namespace: openshift-apiserver
        container_name: 
        label_selector: "app=openshift-apiserver-a"
  startx_zone_outage.yaml: |
    zone_outage:
      cloud_type: aws
      duration: 120
      vpc_id: vpc-0a06fb82c7deef947
      subnet_id: ["subnet-08c7f41c25bc51eb9"]
      # subnet_id: ["subnet-08c7f41c25bc51eb9", "subnet-04441df42fd95bfbd", "subnet-092a12092c3205e17"]
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-kubeconfig
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-kubeconfig"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
{{- if eq .Values.kraken.kubeconfig.mode "file" }}
  config: |
    {{- .Values.kraken.kubeconfig.file | nindent 4 }}
{{- end }}
{{- if eq .Values.kraken.kubeconfig.mode "token" }}
  config: |
    kind: Config
    apiVersion: v1
    current-context: default
    clusters:
    - name: default
      cluster:
        insecure-skip-tls-verify: true
        server: "{{ .Values.kraken.kubeconfig.token.server }}"
    users:
    - name: default
      user:
        token: "{{ .Values.kraken.kubeconfig.token.token }}"
    contexts:
    - name: default
      context:
        cluster: default
        namespace: default
        user: default
    preferences: {}
{{- end }}
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-logging
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-logging"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pod_scenarios:
          - - scenarios/startx_openshift_logging.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-monitoring
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-monitoring"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pod_scenarios:
          - - scenarios/startx_openshift_monitoring.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-machines
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-machines"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pod_scenarios:
          - - scenarios/startx_openshift_machines.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 300
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-apiserver
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-apiserver"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pod_scenarios:
          - - scenarios/startx_openshift-apiserver.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 180
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-etcd
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-etcd"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pod_scenarios:
          - - scenarios/startx_openshift_etcd_pod.yml
        - container_scenarios: 
          - - scenarios/startx_openshift_etcd_container.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 480
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-releasemonkey
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-releasemonkey"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pod_scenarios:
          - - scenarios/startx_openshift_podmonkey.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 240
        iterations: 2
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-time
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-time"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - time_scenarios: 
          - scenarios/startx_time_pod.yml
          - scenarios/startx_time_node.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 420
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-nodes
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-nodes"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - node_scenarios: 
          - scenarios/startx_node_master.yml
          - scenarios/startx_node_infra.yml
          - scenarios/startx_node_app.yml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 600
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-network
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-network"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - network_chaos:
          - scenarios/startx_node_app_network_chaos.yaml
          - scenarios/startx_node_infra_network_chaos.yaml
          - scenarios/startx_node_master_network_chaos.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 300
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-zone
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-zone"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - zone_outages:
          - scenarios/startx_zone_outage.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 600
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-storage
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-storage"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - pvc_scenarios:
          - scenarios/startx_pvc_fruitapp1-preprod-postgresql.yaml
          - scenarios/startx_pvc_fruitapp2-prod-postgresql.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 60
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-chaos-cluster
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-chaos-cluster"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
        - cluster_shut_down_scenarios:
          - - scenarios/cluster_shut_down_scenario.yml
          # - scenarios/post_action_shut_down.py
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 600
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-all
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-all"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  config.yaml: |-
    kraken:
      distribution: openshift
      kubeconfig_path: /root/.kube/config
      exit_on_failure: False
      port: 8081
      publish_kraken_status: True
      signal_state: RUN
      litmus_version: v1.13.6
      litmus_uninstall: False
      litmus_uninstall_before_run: False
      chaos_scenarios:
      - pod_scenarios:
        - - scenarios/startx_openshift_logging.yml
        - - scenarios/startx_openshift_monitoring.yml
        - - scenarios/startx_openshift_machines.yml
        - - scenarios/startx_openshift-apiserver.yml
        - - scenarios/startx_openshift_etcd_pod.yml
        - - scenarios/startx_openshift_podmonkey.yml
      - container_scenarios: 
        - - scenarios/startx_openshift_etcd_container.yml
      - time_scenarios: 
        - scenarios/startx_time_pod.yml
        - scenarios/startx_time_node.yml
      - node_scenarios: 
        - scenarios/startx_node_master.yml
        - scenarios/startx_node_infra.yml
        - scenarios/startx_node_app.yml
      - network_chaos:
        - scenarios/startx_node_app_network_chaos.yaml
        - scenarios/startx_node_infra_network_chaos.yaml
        - scenarios/startx_node_master_network_chaos.yaml
      - zone_outages:
        - scenarios/startx_zone_outage.yaml
      - cluster_shut_down_scenarios:
        - - scenarios/cluster_shut_down_scenario.yml
        # - scenarios/post_action_shut_down.py
      - application_outages:
        - scenarios/app_outage.yaml
      - pvc_scenarios:
        - scenarios/pvc_scenario.yaml
    cerberus:
        cerberus_enabled: True
        cerberus_url: http://cerberus.{{ $.Values.cerberus_project.project.name }}.svc.cluster.local:8080
        check_applicaton_routes: True
    performance_monitoring:
        deploy_dashboards: False
        repo: "https://github.com/cloud-bulldozer/performance-dashboards.git"
        kube_burner_binary_url: "https://github.com/cloud-bulldozer/kube-burner/releases/download/v0.9.1/kube-burner-0.9.1-Linux-x86_64.tar.gz"
        capture_metrics: True
        config_path: config/kube_burner.yaml
        metrics_profile_path: config/metrics-aggregated.yaml
        prometheus_url:
        prometheus_bearer_token:
        uuid:
        enable_alerts: False
        alert_profile: config/alerts
    tunings:
        wait_duration: 420
        iterations: 1
        daemon_mode: False
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kraken-config-common
  namespace: "{{- $namespace -}}"
  labels:
    app.kubernetes.io/name: "kraken-config-common"
    {{- include "example-chaos.labels" $ | nindent 4 }}
  annotations:
    {{- include "example-chaos.annotations" $ | nindent 4 }}
data:
  startx_alerts.yaml: |-
    - expr: avg_over_time(histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))[5m:]) > 0.01
      description: 5 minutes avg. etcd fsync latency on {{`{{`}}$labels.pod{{`}}`}} higher than 10ms {{`{{`}}$value{{`}}`}}
      severity: error
    - expr: avg_over_time(histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))[5m:]) > 0.1
      description: 5 minutes avg. etcd netowrk peer round trip on {{`{{`}}$labels.pod{{`}}`}} higher than 100ms {{`{{`}}$value{{`}}`}}
      severity: info
    - expr: increase(etcd_server_leader_changes_seen_total[2m]) > 0
      description: etcd leader changes observed
      severity: critical
  kube_burner.yaml: |-
    ---
    global:
      writeToFile: true
      metricsDirectory: collected-metrics
      measurements:
        - name: podLatency
          esIndex: kraken
      indexerConfig:
        enabled: true
        esServers: [http://elasticsearch.openshift-logging.svc.cluster.local:9200]
        insecureSkipVerify: true
        defaultIndex: kraken
        type: elastic
  metrics-aggregated.yaml: |-
    metrics:
    # API server
      - query: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{apiserver="kube-apiserver", verb!~"WATCH", subresource!="log"}[2m])) by (verb,resource,subresource,instance,le)) > 0
        metricName: API99thLatency
      - query: sum(irate(apiserver_request_total{apiserver="kube-apiserver",verb!="WATCH",subresource!="log"}[2m])) by (verb,instance,resource,code) > 0
        metricName: APIRequestRate
      - query: sum(apiserver_current_inflight_requests{}) by (request_kind) > 0
        metricName: APIInflightRequests
    # Container & pod metrics
      - query: (sum(container_memory_rss{name!="",container!="POD",namespace=~"openshift-(etcd|oauth-apiserver|.*apiserver|ovn-kubernetes|sdn|ingress|authentication|.*controller-manager|.*scheduler)"}) by (container, pod, namespace, node) and on (node) kube_node_role{role="master"}) > 0
        metricName: containerMemory-Masters
      - query: (sum(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-(etcd|oauth-apiserver|sdn|ovn-kubernetes|.*apiserver|authentication|.*controller-manager|.*scheduler)"}[2m]) * 100) by (container, pod, namespace, node) and on (node) kube_node_role{role="master"}) > 0
        metricName: containerCPU-Masters
      - query: (sum(irate(container_cpu_usage_seconds_total{pod!="",container="prometheus",namespace="openshift-monitoring"}[2m]) * 100) by (container, pod, namespace, node) and on (node) kube_node_role{role="infra"}) > 0
        metricName: containerCPU-Prometheus
      - query: (avg(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress)"}[2m]) * 100 and on (node) kube_node_role{role="worker"}) by (namespace, container)) > 0
        metricName: containerCPU-AggregatedWorkers
      - query: (avg(irate(container_cpu_usage_seconds_total{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress|monitoring|image-registry|logging)"}[2m]) * 100 and on (node) kube_node_role{role="infra"}) by (namespace, container)) > 0
        metricName: containerCPU-AggregatedInfra
      - query: (sum(container_memory_rss{pod!="",namespace="openshift-monitoring",name!="",container="prometheus"}) by (container, pod, namespace, node) and on (node) kube_node_role{role="infra"}) > 0
        metricName: containerMemory-Prometheus
      - query: avg(container_memory_rss{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress)"} and on (node) kube_node_role{role="worker"}) by (container, namespace)
        metricName: containerMemory-AggregatedWorkers
      - query: avg(container_memory_rss{name!="",container!="POD",namespace=~"openshift-(sdn|ovn-kubernetes|ingress|monitoring|image-registry|logging)"} and on (node) kube_node_role{role="infra"}) by (container, namespace)
        metricName: containerMemory-AggregatedInfra
    # Node metrics
      - query: (sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")) > 0
        metricName: nodeCPU-Masters
      - query: (avg((sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))) by (mode)) > 0
        metricName: nodeCPU-AggregatedWorkers
      - query: (avg((sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))) by (mode)) > 0
        metricName: nodeCPU-AggregatedInfra
      - query: avg(node_memory_MemAvailable_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeMemoryAvailable-Masters
      - query: avg(node_memory_MemAvailable_bytes and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryAvailable-AggregatedWorkers
      - query: avg(node_memory_MemAvailable_bytes and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryAvailable-AggregatedInfra
      - query: avg(node_memory_Active_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeMemoryActive-Masters
      - query: avg(node_memory_Active_bytes and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryActive-AggregatedWorkers
      - query: avg(avg(node_memory_Active_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryActive-AggregatedInfra
      - query: avg(node_memory_Cached_bytes) by (instance) + avg(node_memory_Buffers_bytes) by (instance) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeMemoryCached+nodeMemoryBuffers-Masters
      - query: avg(node_memory_Cached_bytes + node_memory_Buffers_bytes and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryCached+nodeMemoryBuffers-AggregatedWorkers
      - query: avg(node_memory_Cached_bytes + node_memory_Buffers_bytes and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)"))
        metricName: nodeMemoryCached+nodeMemoryBuffers-AggregatedInfra
      - query: irate(node_network_receive_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: rxNetworkBytes-Masters
      - query: avg(irate(node_network_receive_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: rxNetworkBytes-AggregatedWorkers
      - query: avg(irate(node_network_receive_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: rxNetworkBytes-AggregatedInfra
      - query: irate(node_network_transmit_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: txNetworkBytes-Masters
      - query: avg(irate(node_network_transmit_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: txNetworkBytes-AggregatedWorkers
      - query: avg(irate(node_network_transmit_bytes_total{device=~"^(ens|eth|bond|team).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: txNetworkBytes-AggregatedInfra
      - query: rate(node_disk_written_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeDiskWrittenBytes-Masters
      - query: avg(rate(node_disk_written_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskWrittenBytes-AggregatedWorkers
      - query: avg(rate(node_disk_written_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskWrittenBytes-AggregatedInfra
      - query: rate(node_disk_read_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
        metricName: nodeDiskReadBytes-Masters
      - query: avg(rate(node_disk_read_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="worker"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskReadBytes-AggregatedWorkers
      - query: avg(rate(node_disk_read_bytes_total{device!~"^(dm|rb).*"}[2m]) and on (instance) label_replace(kube_node_role{role="infra"}, "instance", "$1", "node", "(.+)")) by (device)
        metricName: nodeDiskReadBytes-AggregatedInfra
    # Etcd metrics
      - query: sum(rate(etcd_server_leader_changes_seen_total[2m]))
        metricName: etcdLeaderChangesRate
      - query: etcd_server_is_leader > 0
        metricName: etcdServerIsLeader
      - query: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[2m]))
        metricName: 99thEtcdDiskBackendCommitDurationSeconds
      - query: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m]))
        metricName: 99thEtcdDiskWalFsyncDurationSeconds
      - query: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[5m]))
        metricName: 99thEtcdRoundTripTimeSeconds
      - query: etcd_mvcc_db_total_size_in_bytes
        metricName: etcdDBPhysicalSizeBytes
      - query: etcd_mvcc_db_total_size_in_use_in_bytes
        metricName: etcdDBLogicalSizeBytes
      - query: sum by (cluster_version)(etcd_cluster_version)
        metricName: etcdVersion
        instant: true
      - query: sum(rate(etcd_object_counts{}[5m])) by (resource) > 0
        metricName: etcdObjectCount
      - query: histogram_quantile(0.99,sum(rate(etcd_request_duration_seconds_bucket[2m])) by (le,operation,apiserver)) > 0
        metricName: P99APIEtcdRequestLatency
    # Cluster metrics
      - query: count(kube_namespace_created)
        metricName: namespaceCount
      - query: sum(kube_pod_status_phase{}) by (phase)
        metricName: podStatusCount
      - query: count(kube_secret_info{})
        metricName: secretCount
      - query: count(kube_deployment_labels{})
        metricName: deploymentCount
      - query: count(kube_configmap_info{})
        metricName: configmapCount
      - query: count(kube_service_info{})
        metricName: serviceCount
      - query: kube_node_role
        metricName: nodeRoles
        instant: true
      - query: sum(kube_node_status_condition{status="true"}) by (condition)
        metricName: nodeStatus
      - query: (sum(rate(container_fs_writes_bytes_total{container!="",device!~".+dm.+"}[5m])) by (device, container, node) and on (node) kube_node_role{role="master"}) > 0
        metricName: containerDiskUsage
      - query: cluster_version{type="completed"}
        metricName: clusterVersion
        instant: true
    # Golang metrics
      - query: go_memstats_heap_alloc_bytes{job=~"apiserver|api|etcd"}
        metricName: goHeapAllocBytes
      - query: go_memstats_heap_inuse_bytes{job=~"apiserver|api|etcd"}
        metricName: goHeapInuseBytes
      - query: go_gc_duration_seconds{job=~"apiserver|api|etcd",quantile="1"}
        metricName: goGCDurationSeconds
{{- end -}}
